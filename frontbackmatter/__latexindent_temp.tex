%*******************************************************
% Abstract
\chapter*{Abstract}\label{ch:abstract}
%*******************************************************

In the last decade, we have witnessed a myriad of astonishing
successes in Deep Learning.
Despite those many successes, we may again be climbing a peak
of inflated expectations.
If in the past, the false solution was to ``throw computation
power at problems'', today we try ``throwing data''.
Such behaviour has triggered a winner-takes-all
rush for data among a handful of large corporations,
raising concerns about privacy and concentration of power.
We know for a fact, however, that learning from way fewer samples is possible: humans show a much better generalisation ability than our current state of the art artificial intelligence.
To achieve such needed generalisation power, we must
better understand how learning happens in deep neural networks.
The practice of modern machine learning has outpaced its theoretical development, deep learning models present generalisation capabilities
unpredicted by current machine learning theory.
There is yet no established new general theory of learning which handles
this problem.
In 2015, Naftali Tishby and Noga Zaslavsky published a
seminal theory of learning based on the information-theoretical concept
of the bottleneck principle.
This dissertation aims to investigate the scattered efforts of using the
information bottleneck principle to explain the generalisation capabilities
of deep neural networks and consolidate them into a comprehensive digest
of this new general deep learning theory.
